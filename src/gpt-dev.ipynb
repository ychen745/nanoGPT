{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"158AS8-vJQJ2m8rVT8w7okMYHcZk-a6T2","authorship_tag":"ABX9TyNivSSIUERMNxXZD6RBvps/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YM9-X9ZAViqQ"},"outputs":[],"source":["import os\n","import shutil\n","import torch\n","\n","input_file = '/content/drive/MyDrive/nanoGPT/data/input.txt'\n","\n","with open(input_file, 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","print(\"length of dataset in characters:\", len(text))"]},{"cell_type":"code","source":["# get a set of all chars in text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"id":"5qBL2JOjap2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build dictionaries between chars and ints\n","dict_stoi = {ch: i for i, ch in enumerate(chars)}\n","dict_itos = {i: ch for i, ch in enumerate(chars)}\n","encode = lambda s: [dict_stoi[c] for c in s]\n","decode = lambda l: ''.join([dict_itos[i] for i in l])\n","\n","# print(encode(\"hii there\"))\n","# print(decode(encode(\"hii there\")))"],"metadata":{"id":"ouy3bBAHV6_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)"],"metadata":{"id":"xtadK2jvcB_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n"],"metadata":{"id":"GlGwP2ehc6GY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","# train_data[:block_size+1]"],"metadata":{"id":"rXHLRlGYdUPq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = data[:block_size]\n","y = data[1:block_size+1]\n","for t in range(block_size):\n","  context = x[:t+1]\n","  target = y[t]\n","  print(f\"When input is {context} the target is {target}\")"],"metadata":{"id":"l7Gp2Axrduei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4\n","block_size = 8\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size, ))\n","    x = torch.stack([data[i:i + block_size] for i in ix])\n","    y = torch.stack([data[i+1:i + block_size + 1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)"],"metadata":{"id":"fVbNm9ldiugp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is B*T array of current context\n","        for _ in range(max_new_tokens):\n","            # get predictions\n","            logits, loss = self(idx)\n","            # focus only on the last timestep\n","            logits = logits[:, -1, :]\n","            # apply softmax to get largest probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1)\n","        return idx\n","\n","model = BigramLanguageModel(vocab_size)\n","logits, loss = model(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","idx = torch.zeros((1, 1), dtype=torch.long)\n","print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))"],"metadata":{"id":"naoR9QNq2lbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","batch_size = 32\n","\n","for steps in range(10000):\n","    xb, yb = get_batch('train')\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())"],"metadata":{"id":"qSkhr6uOGkr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(decode(model.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"id":"HJGTC_BYHU8-"},"execution_count":null,"outputs":[]}]}